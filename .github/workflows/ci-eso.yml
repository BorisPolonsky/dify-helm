name: CI - External Secrets Operator Tests

on:
  push:
    branches: [main, develop, feat/support-externa-secret]
  pull_request:
    branches: [main, develop]

jobs:
  # Stage 1: Pre-pull all images and prepare infrastructure
  prepare-images:
    runs-on: ubuntu-latest
    outputs:
      cache-hit: ${{ steps.cache-check.outputs.cache-hit }}
      image-cache-key: ${{ steps.cache-key.outputs.key }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # Generate unified cache key
      - name: Generate cache key
        id: cache-key
        run: |
          CACHE_KEY="${{ runner.os }}-minikube-images-${{ hashFiles('ci/values/*.yaml', 'charts/dify/Chart.yaml', 'charts/dify/Chart.lock') }}"
          echo "key=$CACHE_KEY" >> $GITHUB_OUTPUT
          echo "Generated cache key: $CACHE_KEY"

      # Check image cache with optimized restore strategy
      - name: Check Docker images cache
        id: cache-check
        uses: actions/cache@v4
        with:
          path: ~/.minikube/cache/images
          key: ${{ steps.cache-key.outputs.key }}
          restore-keys: |
            ${{ runner.os }}-minikube-images-${{ hashFiles('charts/dify/Chart.yaml', 'charts/dify/Chart.lock') }}-
            ${{ runner.os }}-minikube-images-${{ hashFiles('charts/dify/Chart.yaml') }}-
            ${{ runner.os }}-minikube-images-

      # Only run if cache miss
      - name: Setup minikube for image caching
        if: steps.cache-check.outputs.cache-hit != 'true'
        uses: medyagh/setup-minikube@latest
        with:
          cache: true

      # Cache all images (unified list for all scenarios)
      - name: Cache all Docker images
        if: steps.cache-check.outputs.cache-hit != 'true'
        run: |
          echo "Pre-pulling all Docker images to minikube cache..."

          # Extract images from values files using script
          chmod +x ci/scripts/extract-images.sh
          EXTRACTED_IMAGES=$(ci/scripts/extract-images.sh values-eso.yaml github-actions 2>/dev/null)

          # Add additional images for ESO scenarios
          ADDITIONAL_IMAGES="hashicorp/vault:latest bitnami/minio:2024-debian-12-r2 semitechnologies/weaviate:1.19.1 bitnami/elasticsearch:8.11.0"

          # Combine extracted and additional images
          ALL_IMAGES="$EXTRACTED_IMAGES $ADDITIONAL_IMAGES"

          echo "ALL_IMAGES=$ALL_IMAGES" >> $GITHUB_ENV

          # Display extracted images for verification
          echo "Extracted Images from values-eso.yaml:"
          ci/scripts/extract-images.sh values-eso.yaml list
          echo ""
          echo "Additional ESO-specific Images:"
          echo "hashicorp/vault:latest"
          echo "bitnami/minio:2024-debian-12-r2"
          echo "semitechnologies/weaviate:1.19.1"
          echo "bitnami/elasticsearch:8.11.0"

          IMAGE_COUNT=$(echo "$ALL_IMAGES" | wc -w)
          echo "Total images to cache: $IMAGE_COUNT"

          # Pull images in parallel (limit to 3 concurrent)
          echo "$ALL_IMAGES" | tr ' ' '\n' | grep -v '^[[:space:]]*$' | while read -r image; do
            {
              echo "Pulling $image..."
              if minikube image pull "$image" 2>/dev/null; then
                echo "Successfully cached $image"
              else
                echo "Failed to cache $image (might already exist)"
              fi
            } &

            # Limit concurrent pulls
            if (( $(jobs -r | wc -l) >= 3 )); then
              wait -n
            fi
          done

          # Wait for all remaining jobs
          wait

          echo "Image caching completed"

          # Show cached images
          echo "Cached images in minikube:"
          minikube image ls --format table | head -20 || true

  # Stage 2: Matrix testing with cached images
  test-matrix:
    needs: prepare-images
    runs-on: ubuntu-latest
    strategy:
      fail-fast: false
      matrix:
        include:
          - config_name: "legacy: using built-in PostgreSQL and built-in Redis"
            values_file: "values-legacy.yaml"
            use_eso: false
            use_external_pg: false
            use_external_s3: false
            use_external_es: false

          - config_name: "legacy: using external PostgreSQL and built-in Redis"
            values_file: "values-legacy-external-pg.yaml"
            use_eso: false
            use_external_pg: true
            use_external_s3: false
            use_external_es: false

          - config_name: "legacy: using external S3 and built-in PostgreSQL and Redis"
            values_file: "values-legacy-external-s3.yaml"
            use_eso: false
            use_external_pg: false
            use_external_s3: true
            use_external_es: false

          - config_name: "external-secrets: built-in PostgreSQL and built-in Redis(use existing secret)"
            values_file: "values-eso.yaml"
            use_eso: true
            use_external_pg: false
            use_external_s3: false
            use_external_es: false

          - config_name: "external-secrets: using external PostgreSQL and built-in Redis(use exsting secret)"
            values_file: "values-eso-external-pg.yaml"
            use_eso: true
            use_external_pg: true
            use_external_s3: false
            use_external_es: false

          - config_name: "external-secrets: using external S3 with built-in PostgreSQL and Redis"
            values_file: "values-eso-external-s3.yaml"
            use_eso: true
            use_external_pg: false
            use_external_s3: true
            use_external_es: false

          - config_name: "external-secrets: using external S3 and external PostgreSQL"
            values_file: "values-eso-external-s3-pg.yaml"
            use_eso: true
            use_external_pg: true
            use_external_s3: true
            use_external_es: false

          - config_name: "external-secrets: using external Elasticsearch"
            values_file: "values-eso-external-es.yaml"
            use_eso: true
            use_external_pg: false
            use_external_s3: false
            use_external_es: true

    name: ${{ matrix.config_name }}

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      # Restore all caches with optimized restore strategies
      - name: Restore Docker images cache
        uses: actions/cache@v4
        with:
          path: ~/.minikube/cache/images
          key: ${{ needs.prepare-images.outputs.image-cache-key }}
          restore-keys: |
            ${{ runner.os }}-minikube-images-${{ hashFiles('charts/dify/Chart.yaml', 'charts/dify/Chart.lock') }}-
            ${{ runner.os }}-minikube-images-${{ hashFiles('charts/dify/Chart.yaml') }}-
            ${{ runner.os }}-minikube-images-

      - name: Restore Helm repositories cache
        uses: actions/cache@v4
        with:
          path: ~/.cache/helm/repository
          key: ${{ runner.os }}-helm-repos-${{ hashFiles('charts/dify/Chart.yaml') }}
          restore-keys: |
            ${{ runner.os }}-helm-repos-

      - name: Restore Helm chart dependencies cache
        uses: actions/cache@v4
        with:
          path: charts/dify/charts
          key: ${{ runner.os }}-helm-deps-${{ hashFiles('charts/dify/Chart.lock', 'charts/dify/Chart.yaml') }}
          restore-keys: |
            ${{ runner.os }}-helm-deps-${{ hashFiles('charts/dify/Chart.yaml') }}-
            ${{ runner.os }}-helm-deps-

      # Each job runs in its own VM, so we need to setup minikube and helm again
      - name: Setup minikube
        uses: medyagh/setup-minikube@latest
        with:
          cache: true

      - name: Setup Helm
        uses: azure/setup-helm@v4
        with:
          version: "v3.13.0"

      # Add Helm repositories (leveraging cache from prepare-images stage)
      - name: Add Helm repositories
        run: |
          echo "Adding Helm repositories..."
          helm repo add external-secrets https://charts.external-secrets.io || true
          helm repo add hashicorp https://helm.releases.hashicorp.com || true
          helm repo add bitnami https://charts.bitnami.com/bitnami || true
          helm repo update

      - name: Verify cached images
        run: |
          echo "Verifying cached images for scenario: ${{ matrix.config_name }}"
          echo "Images available in minikube:"
          minikube image ls --format table | head -20 || true

      - name: Install chart dependencies
        run: |
          cd charts/dify
          if [ ! -d "charts" ] || [ ! "$(ls -A charts)" ]; then
            echo "Installing Helm chart dependencies..."
            helm dependency update
          else
            echo "Helm chart dependencies already cached"
          fi

      - name: Install External Secrets Operator
        if: matrix.use_eso
        run: |
          echo "Installing External Secrets Operator..."
          helm install external-secrets external-secrets/external-secrets \
            --namespace external-secrets-system \
            --create-namespace \
            --wait \
            --timeout 120s

      - name: Install Vault
        if: matrix.use_eso
        run: |
          echo "Installing Vault..."
          helm install vault hashicorp/vault \
            --set "server.dev.enabled=true" \
            --set "server.dev.devRootToken=dev-only-token" \
            --wait \
            --timeout 120s

      - name: Wait for External Secrets and Vault to be ready
        if: matrix.use_eso
        run: |
          echo "Waiting for External Secrets Operator components..."
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=external-secrets -n external-secrets-system --timeout=120s
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=external-secrets-webhook -n external-secrets-system --timeout=120s
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=external-secrets-cert-controller -n external-secrets-system --timeout=120s

          echo "Waiting for Vault to be ready..."
          kubectl wait --for=condition=ready pod -l app.kubernetes.io/name=vault --timeout=120s

          echo "All components are ready!"

      - name: Setup Vault secrets
        if: matrix.use_eso
        run: |
          chmod +x ci/scripts/setup-vault-secrets.sh
          ci/scripts/setup-vault-secrets.sh

      - name: Validate Vault secret paths
        if: matrix.use_eso
        run: |
          chmod +x ci/scripts/validate-vault-paths.sh
          ci/scripts/validate-vault-paths.sh

      - name: Create test ClusterSecretStore
        if: matrix.use_eso
        run: |
          chmod +x ci/scripts/create-test-clustersecretstore.sh
          ci/scripts/create-test-clustersecretstore.sh

      - name: Install external PostgreSQL
        if: matrix.use_external_pg
        run: |
          echo "Installing external PostgreSQL..."
          helm install external-postgres bitnami/postgresql \
            --set auth.postgresPassword="difyai123456" \
            --set auth.database="dify" \
            --set primary.service.type="ClusterIP" \
            --set primary.service.ports.postgresql=5432 \
            --wait \
            --timeout 300s

      - name: Setup external PostgreSQL DNS
        if: matrix.use_external_pg
        run: |
          chmod +x ci/scripts/setup-external-postgres-dns.sh
          ci/scripts/setup-external-postgres-dns.sh

      - name: Install MinIO
        if: matrix.use_external_s3
        run: |
          echo "Installing MinIO for S3 testing..."
          helm install minio bitnami/minio \
            --set auth.rootUser="minio-root" \
            --set auth.rootPassword="minio123456" \
            --set defaultBuckets="difyai" \
            --set service.type="ClusterIP" \
            --set service.ports.api=9000 \
            --wait \
            --timeout 300s

      - name: Install Elasticsearch
        if: matrix.use_external_es
        run: |
          echo "Installing Elasticsearch for vector database testing..."
          helm install external-elasticsearch bitnami/elasticsearch \
            --set auth.elasticPassword="elasticsearch123456" \
            --set master.replicaCount=1 \
            --set data.replicaCount=0 \
            --set coordinating.replicaCount=0 \
            --set ingest.replicaCount=0 \
            --set master.persistence.enabled=false \
            --set service.type="ClusterIP" \
            --set service.ports.restAPI=9200 \
            --wait \
            --timeout 300s

      - name: Test Helm template rendering
        run: |
          chmod +x ci/scripts/test-helm-template.sh
          ci/scripts/test-helm-template.sh "${{ matrix.values_file }}" "${{ matrix.config_name }}"

      - name: Deploy and test
        env:
          HELM_TIMEOUT: "700s"
          POD_READY_TIMEOUT: "200s"
        run: |
          chmod +x ci/scripts/deploy-and-test.sh
          ci/scripts/deploy-and-test.sh "${{ matrix.values_file }}"

      - name: Cache cleanup on failure
        if: failure()
        run: |
          echo "Cleaning up caches due to job failure..."
          rm -rf charts/dify/charts

      - name: Cache cleanup on failure
        if: failure()
        run: |
          echo "Cleaning up caches due to job failure..."
          rm -rf charts/dify/charts

  # Summary job
  test-summary:
    runs-on: ubuntu-latest
    name: Test Summary
    needs: test-matrix
    if: always()
    steps:
      - name: Check test results
        run: |
          echo "Test Results Summary"
          echo "===================="
          if [[ "${{ needs.test-matrix.result }}" != "success" ]]; then
            echo "One or more test configurations failed"
            echo "Please check the individual job logs for details"
            exit 1
          else
            echo "All test configurations passed successfully!"
          fi
          echo ""
          echo "Tested Configurations:"
          echo "- ESO + Built-in PostgreSQL"
          echo "- ESO + External PostgreSQL"
          echo "- ESO + External S3 bucket"
          echo "- ESO + External S3 + External PostgreSQL"
          echo "- Legacy + Built-in PostgreSQL"
          echo "- Legacy + External PostgreSQL"
          echo "- Legacy + External S3"
          echo ""
          echo "CI Pipeline completed successfully!"
